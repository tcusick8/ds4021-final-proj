{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and clean\n",
    "X_train = pd.read_csv('~/Downloads/X_train.csv')\n",
    "y_train = pd.read_csv('~/Downloads/y_train.csv')\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "train_clean = train.dropna()\n",
    "X_train_clean = train_clean.iloc[:, :-1]\n",
    "y_train_clean = train_clean.iloc[:, -1]\n",
    "\n",
    "y_coords = y_train_clean.apply(ast.literal_eval)\n",
    "y_clean = np.vstack(y_coords.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 100,000\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "sample_size = 100000\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_train_clean), size=sample_size, replace=False)\n",
    "X_sample = X_train_clean.iloc[sample_indices].reset_index(drop=True)\n",
    "y_sample = y_clean[sample_indices]\n",
    "\n",
    "print(f\"Training samples: {len(X_sample):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking categorical columns:\n",
      "  Case Number: 99999 unique values\n",
      "  Date: 84854 unique values\n",
      "  Block: 23088 unique values\n",
      "  IUCR: 297 unique values\n",
      "  Primary Type: 31 unique values\n",
      "  Description: 279 unique values\n",
      "  Arrest: 2 unique values\n",
      "  Domestic: 2 unique values\n",
      "  FBI Code: 26 unique values\n",
      "  Updated On: 1768 unique values\n"
     ]
    }
   ],
   "source": [
    "# Feature selection and encoding\n",
    "# Check unique values in categorical columns\n",
    "print(\"Checking categorical columns:\")\n",
    "for col in X_sample.select_dtypes(exclude=['number']).columns:\n",
    "    n_unique = X_sample[col].nunique()\n",
    "    print(f\"  {col}: {n_unique} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric columns : ['Year']\n",
      "Categorical columns : ['IUCR', 'Primary Type', 'Description', 'Arrest', 'Domestic', 'FBI Code']\n",
      "  Dropping IUCR: 297 unique values (too many)\n",
      "  Keeping Primary Type: 31 unique values\n",
      "  Dropping Description: 279 unique values (too many)\n",
      "  Keeping Arrest: 2 unique values\n",
      "  Keeping Domestic: 2 unique values\n",
      "  Keeping FBI Code: 26 unique values\n",
      "\n",
      "Final feature set: 5 features\n",
      "Columns: ['Year', 'Primary Type', 'Arrest', 'Domestic', 'FBI Code']\n"
     ]
    }
   ],
   "source": [
    "# Prepare features - keep less unique categoricals\n",
    "# Drop highly unique columns\n",
    "high_cardinality_cols = ['ID', 'Case Number', 'Date', 'Block', 'Updated On']\n",
    "X_selected = X_sample.drop(columns=high_cardinality_cols, errors='ignore')\n",
    "# Drop location-related columns\n",
    "X_selected = X_selected.drop(columns=['Ward', 'Community Area', 'Beat', 'District'], errors='ignore')\n",
    "\n",
    "# Separate numeric and categorical\n",
    "numeric_cols = X_selected.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols = X_selected.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric columns : {numeric_cols}\")\n",
    "print(f\"Categorical columns : {categorical_cols}\")\n",
    "\n",
    "# Check if we should keep categorical columns based on uniqueness\n",
    "keep_categorical = []\n",
    "for col in categorical_cols:\n",
    "    n_unique = X_selected[col].nunique()\n",
    "    if n_unique <= 100:  # Keep if <= 100 unique values\n",
    "        keep_categorical.append(col)\n",
    "        print(f\"  Keeping {col}: {n_unique} unique values\")\n",
    "    else:\n",
    "        print(f\"  Dropping {col}: {n_unique} unique values (too many)\")\n",
    "\n",
    "# Create final feature set\n",
    "X_processed = X_selected[numeric_cols].copy()\n",
    "\n",
    "# Label encode categorical columns \n",
    "label_encoders = {}\n",
    "for col in keep_categorical:\n",
    "    le = LabelEncoder()\n",
    "    X_processed[col] = le.fit_transform(X_selected[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"\\nFinal feature set: {X_processed.shape[1]} features\")\n",
    "print(f\"Columns: {X_processed.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search sample: 10,000\n",
      "Features: 5\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] END max_depth=10, min_samples_split=20, n_estimators=30; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_split=20, n_estimators=30; total time=   0.0s\n",
      "[CV] END max_depth=10, min_samples_split=20, n_estimators=30; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_split=20, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_split=20, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_split=20, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_split=50, n_estimators=30; total time=   0.0s\n",
      "[CV] END max_depth=10, min_samples_split=50, n_estimators=30; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_split=50, n_estimators=30; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_split=50, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_split=50, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_split=50, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=15, min_samples_split=20, n_estimators=30; total time=   0.0s\n",
      "[CV] END max_depth=15, min_samples_split=20, n_estimators=30; total time=   0.0s\n",
      "[CV] END max_depth=15, min_samples_split=20, n_estimators=30; total time=   0.0s\n",
      "[CV] END max_depth=15, min_samples_split=20, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=15, min_samples_split=20, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=15, min_samples_split=20, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=15, min_samples_split=50, n_estimators=30; total time=   0.1s\n",
      "[CV] END max_depth=15, min_samples_split=50, n_estimators=30; total time=   0.1s\n",
      "[CV] END max_depth=15, min_samples_split=50, n_estimators=30; total time=   0.0s\n",
      "[CV] END max_depth=15, min_samples_split=50, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=15, min_samples_split=50, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=15, min_samples_split=50, n_estimators=50; total time=   0.1s\n",
      "\n",
      "Best parameters: {'max_depth': 15, 'min_samples_split': 50, 'n_estimators': 50}\n",
      "Best CV score (neg MSE): -0.0054\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "grid_sample_size = 10000\n",
    "grid_indices = np.random.choice(len(X_processed), size=grid_sample_size, replace=False)\n",
    "X_grid = X_processed.iloc[grid_indices]\n",
    "y_grid = y_sample[grid_indices]\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [30, 50],\n",
    "    'max_depth': [10, 15],\n",
    "    'min_samples_split': [20, 50]\n",
    "}\n",
    "\n",
    "rf_grid = RandomForestRegressor(\n",
    "    max_features='sqrt',  # Changed from fixed 5\n",
    "    min_samples_leaf=20,\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Grid search sample: {grid_sample_size:,}\")\n",
    "print(f\"Features: {X_processed.shape[1]}\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf_grid,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "grid_search.fit(X_grid, y_grid)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score (neg MSE): {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training completed in 0.01 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "#Train final model\n",
    "best_rf = RandomForestRegressor(\n",
    "    n_estimators=grid_search.best_params_['n_estimators'],\n",
    "    max_depth=grid_search.best_params_['max_depth'],\n",
    "    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=20,\n",
    "    n_jobs=4,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "best_rf.fit(X_processed, y_sample)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n✓ Training completed in {elapsed/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.005366\n",
      "Training R²:  0.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on training data\n",
    "y_pred_train = best_rf.predict(X_processed)\n",
    "train_mse = mean_squared_error(y_sample, y_pred_train)\n",
    "train_r2 = r2_score(y_sample, y_pred_train)\n",
    "\n",
    "print(f\"Training MSE: {train_mse:.6f}\")\n",
    "print(f\"Training R²:  {train_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
